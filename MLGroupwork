import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns


df.head()


print("\nDataset Info:")
print(df.info())

# Summary statistics
print("\nSummary Statistics:")
print(df.describe())

# Check for missing values
print("\nMissing Values:")
print(df.isnull().sum())

# Data types
print("\nData Types:")
print(df.dtypes)

# Univariate Analysis - Histograms
df.hist(figsize=(12, 8), bins=30)
plt.suptitle("Feature Distributions")
plt.show()




corr_matrix = df.corr()
corr_matrix

import seaborn as sns
import matplotlib.pyplot as plt

# Set the size of the figure
plt.figure(figsize=(12, 8))

# Create the heatmap
sns.heatmap(corr_matrix, 
            annot=True,            # Show correlation coefficients
            cmap='coolwarm',       # Color map (you can choose another)
            square=True,           # Make cells square for a cleaner look
            fmt='.2f',             # Format for the annotation text
            cbar=True)             # Show color bar

plt.title('Correlation Matrix')
plt.show()



import pandas as pd
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt

# Define the threshold for strong correlations
threshold = 0.5  # Adjust as needed

# Compute correlation matrix
corr_matrix = df.corr()

# Mask correlations below the threshold
filtered_corr = corr_matrix.where(np.abs(corr_matrix) > threshold, np.nan)

# Find columns where at least one value (excluding diagonal) exceeds the threshold
keep_cols = filtered_corr.dropna(how='all', axis=1).columns  # Keep only relevant columns/rows

# Filter the correlation matrix to include only those columns
filtered_corr = filtered_corr.loc[keep_cols, keep_cols]

# Plot the filtered correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(filtered_corr, 
            annot=True,   # Show values
            cmap='coolwarm', 
            fmt=".2f", 
            mask=np.isnan(filtered_corr),  # Hide NaNs (correlations below threshold)
            linewidths=0.5, 
            square=True)

plt.title(f"Filtered Correlation Matrix (Only Variables with Correlation > {threshold})")
plt.show()

# 2. Flatten the correlation matrix into a long DataFrame
corr_pairs = (
    corr_matrix
    .abs()             # Take absolute value of correlations
    .unstack()         # Convert to a Series with multi-index (row,col)
    .sort_values(ascending=False)
    .drop_duplicates() # Remove duplicate pairs (i.e. mirroring)
)

# 3. Filter by threshold (e.g., 0.5)
threshold = 0.5
strong_pairs = corr_pairs[corr_pairs > threshold]

print(strong_pairs)





from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import GridSearchCV


# Drop unnecessary columns
drop_cols = ["EmployeeCount", "EmployeeNumber", "Over18", "StandardHours"]  # Not useful for prediction
df.drop(columns=drop_cols, inplace=True, errors='ignore')
print(df.Attrition)
# Convert 'Attrition' (target variable) to binary
df["Attrition"] = df["Attrition"].map({"Yes": 1, "No": 0})

# Encode categorical variables
cat_cols = df.select_dtypes(include=["object"]).columns
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# Split data into train & test
X = df.drop("Attrition", axis=1)
y = df["Attrition"]
print(y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)


# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train_scaled, y_train)
y_pred_log = log_reg.predict(X_test_scaled)

# Train Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)


param_grid = {
    "n_estimators": [100, 300, 500],  
    "max_depth": [None, 10, 20],  
    "min_samples_split": [2, 5, 10],  
    "min_samples_leaf": [1, 2, 4],  
    "class_weight": ["balanced"]  # Handle imbalance
}

rf_tuned = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf_tuned, param_grid, cv=5, scoring="recall", n_jobs=-1)
grid_search.fit(X_train, y_train)

best_rf = grid_search.best_estimator_


# Evaluation
print("\nðŸ”¹ Logistic Regression Results")
print("Accuracy:", accuracy_score(y_test, y_pred_log))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_log))
print("Classification Report:\n", classification_report(y_test, y_pred_log))

print("\nðŸ”¹ Random Forest Results")
print("Accuracy:", accuracy_score(y_test, y_pred_rf))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_rf))
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

# ROC Curve
y_pred_prob = rf_model.predict_proba(X_test)[:, 1]
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"Random Forest (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()


print(best_rf)



importances = best_rf.feature_importances_
important_features = pd.Series(importances, index=X_train.columns).sort_values(ascending=False)

print("\nðŸ”¹ Top 10 Important Features")
print(important_features.head(80))

# Plot
plt.figure(figsize=(10, 5))
sns.barplot(x=important_features[:10], y=important_features.index[:10])
plt.title("Top 10 Feature Importances in Random Forest")
plt.show()




drop_cols = [
    "EducationField_Other", "JobRole_Human Resources", "PerformanceRating", "JobRole_Manager",
    "JobRole_Sales Representative", "EducationField_Technical Degree", "EducationField_Marketing",
    "EducationField_Medical"
]

X_train_reduced = X_train.drop(columns=drop_cols)
X_test_reduced = X_test.drop(columns=drop_cols)

rf_tuned = RandomForestClassifier(
    class_weight={0: 1, 1: 4},  # Increase weight for class 1 (attrition)
    max_depth=12,               
    min_samples_split=5,        
    min_samples_leaf=2,         
    n_estimators=500,           
    random_state=42
)

rf_tuned.fit(X_train_reduced, y_train)
y_pred_rf_tuned = rf_tuned.predict(X_test_reduced)

print("\nðŸ”¹ Adjusted Random Forest Performance")
print(classification_report(y_test, y_pred_rf_tuned))




import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, roc_curve, auc
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE

# Load Data
file_path = "Employee_attrition.csv"  # Change to your dataset path
df = pd.read_csv(file_path)

# Drop unnecessary columns
drop_cols = ["EmployeeCount", "EmployeeNumber", "Over18", "StandardHours"]
df.drop(columns=drop_cols, inplace=True)

# Convert 'Attrition' (target variable) to binary
df["Attrition"] = df["Attrition"].map({"Yes": 1, "No": 0})

# Encode categorical variables
df = pd.get_dummies(df, drop_first=True)

# Remove low-importance features
drop_low_importance = [
    "EducationField_Other", "JobRole_Human Resources", "PerformanceRating", "JobRole_Manager",
    "JobRole_Sales Representative", "EducationField_Technical Degree", "EducationField_Marketing",
    "EducationField_Medical"
]
df.drop(columns=[col for col in drop_low_importance if col in df.columns], inplace=True)

# Split data into train & test
X = df.drop("Attrition", axis=1)
y = df["Attrition"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Scale numerical features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_smote)
X_test_scaled = scaler.transform(X_test)

# ðŸš€ **Random Forest with Class Weight Adjustment**
rf_model = RandomForestClassifier(
    class_weight={0: 1, 1: 4},  # Increase weight for attrition cases
    max_depth=12, 
    min_samples_split=5, 
    min_samples_leaf=2, 
    n_estimators=500, 
    random_state=42
)
rf_model.fit(X_train_scaled, y_train_smote)
y_pred_rf = rf_model.predict(X_test_scaled)

# Evaluate Random Forest
print("\nðŸ”¹ Random Forest Performance")
print(classification_report(y_test, y_pred_rf))

# Lower prediction threshold for better recall
y_pred_prob_rf = rf_model.predict_proba(X_test_scaled)[:, 1]
y_pred_rf_adjusted = (y_pred_prob_rf >= 0.3).astype(int)

print("\nðŸ”¹ Random Forest with Lowered Threshold (0.3)")
print(classification_report(y_test, y_pred_rf_adjusted))

# ðŸš€ **XGBoost Model for Comparison**
xgb_model = XGBClassifier(
    scale_pos_weight=4,  # Adjusts for class imbalance
    max_depth=10, 
    learning_rate=0.05,  
    n_estimators=500, 
    colsample_bytree=0.8, 
    subsample=0.8,        
    random_state=42
)
xgb_model.fit(X_train_scaled, y_train_smote)
y_pred_xgb = xgb_model.predict(X_test_scaled)

print("\nðŸ”¹ XGBoost Performance")
print(classification_report(y_test, y_pred_xgb))

# ðŸš€ **ROC Curve for Comparison**
plt.figure(figsize=(8, 6))
for model, y_pred_prob, label in zip(
    [rf_model, xgb_model], 
    [y_pred_prob_rf, xgb_model.predict_proba(X_test_scaled)[:, 1]], 
    ["Random Forest", "XGBoost"]
):
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{label} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Model Comparison")
plt.legend()
plt.show()



import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, roc_curve, auc
from sklearn.linear_model import LogisticRegression

# ðŸš€ **1ï¸âƒ£ Find the Best Threshold**
def find_best_threshold(model, X_test, y_test):
    y_probs = model.predict_proba(X_test)[:, 1]
    thresholds = np.linspace(0.1, 0.9, 9)  # Test multiple thresholds
    results = []

    for t in thresholds:
        y_pred_adjusted = (y_probs >= t).astype(int)
        report = classification_report(y_test, y_pred_adjusted, output_dict=True)
        recall = report["1"]["recall"]
        precision = report["1"]["precision"]
        f1 = report["1"]["f1-score"]
        results.append((t, precision, recall, f1))

    results = np.array(results)
    
    # Plot Threshold vs. Recall & Precision
    plt.figure(figsize=(8, 5))
    plt.plot(results[:, 0], results[:, 1], label="Precision", marker="o")
    plt.plot(results[:, 0], results[:, 2], label="Recall", marker="o")
    plt.plot(results[:, 0], results[:, 3], label="F1-score", marker="o")
    plt.xlabel("Threshold")
    plt.ylabel("Score")
    plt.title("Precision, Recall, and F1-Score vs. Threshold")
    plt.legend()
    plt.show()

    # Return best threshold (where F1-score is highest)
    best_threshold = results[np.argmax(results[:, 3]), 0]
    print(f"\nðŸ”¹ Best Threshold Found: {best_threshold:.2f}")
    return best_threshold

best_threshold_rf = find_best_threshold(rf_model, X_test_scaled, y_test)

# ðŸš€ **2ï¸âƒ£ Train XGBoost with Better Class Balancing**
xgb_optimized = XGBClassifier(
    scale_pos_weight=5,  # Increased weight for attrition cases
    max_depth=10, 
    learning_rate=0.03,  # Slower learning rate for better generalization
    n_estimators=600,  # More boosting rounds
    colsample_bytree=0.8, 
    subsample=0.8,        
    random_state=42
)

xgb_optimized.fit(X_train_scaled, y_train_smote)
y_pred_xgb_opt = xgb_optimized.predict(X_test_scaled)

print("\nðŸ”¹ XGBoost (Optimized) Performance")
print(classification_report(y_test, y_pred_xgb_opt))

# ðŸš€ **3ï¸âƒ£ Train a Weighted Logistic Regression**
log_reg = LogisticRegression(class_weight="balanced", solver="liblinear")
log_reg.fit(X_train_scaled, y_train_smote)
y_pred_log = log_reg.predict(X_test_scaled)

print("\nðŸ”¹ Weighted Logistic Regression Performance")
print(classification_report(y_test, y_pred_log))

# ðŸš€ **4ï¸âƒ£ Final Model Comparison with Best Threshold**
y_pred_rf_final = (rf_model.predict_proba(X_test_scaled)[:, 1] >= best_threshold_rf).astype(int)
print("\nðŸ”¹ Final Random Forest Performance (Using Best Threshold)")
print(classification_report(y_test, y_pred_rf_final))

# ðŸš€ **5ï¸âƒ£ Plot Final ROC Curves**
plt.figure(figsize=(8, 6))
for model, label in zip(
    [rf_model, xgb_optimized, log_reg], 
    ["Random Forest", "XGBoost", "Logistic Regression"]
):
    y_pred_prob = model.predict_proba(X_test_scaled)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f"{label} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Final Model Comparison")
plt.legend()
plt.show()



log_reg_tuned = LogisticRegression(class_weight={0: 1, 1: 5}, solver="liblinear")
log_reg_tuned.fit(X_train_scaled, y_train_smote)
y_pred_log_tuned = log_reg_tuned.predict(X_test_scaled)

print("\nðŸ”¹ Tuned Weighted Logistic Regression Performance")
print(classification_report(y_test, y_pred_log_tuned))


log_reg_tuned = LogisticRegression(class_weight={0: 1, 1: 3}, solver="liblinear")
log_reg_tuned.fit(X_train_scaled, y_train_smote)
y_pred_log_tuned = log_reg_tuned.predict(X_test_scaled)

print("\nðŸ”¹ Tuned Weighted Logistic Regression Performance")
print(classification_report(y_test, y_pred_log_tuned))




y_pred_prob_gb = xgb_model.predict_proba(X_test_scaled)[:, 1]  # Get probability of attrition (class 1)
y_pred_gb_adjusted = (y_pred_prob_gb >= 0.35).astype(int)  # Change threshold from 0.5 to 0.35

# Evaluate Model with New Threshold
print("\nðŸ”¹ Gradient Boosting with Lowered Threshold (0.35)")
print(classification_report(y_test, y_pred_gb_adjusted))


## Gradient boosting

from sklearn.ensemble import GradientBoostingClassifier

gb_model = GradientBoostingClassifier(n_estimators=500, learning_rate=0.05, max_depth=5, random_state=42)
gb_model.fit(X_train_scaled, y_train_smote)
y_pred_gb = gb_model.predict(X_test_scaled)

print("\nðŸ”¹ Gradient Boosting Performance")
print(classification_report(y_test, y_pred_gb))



from sklearn.metrics import classification_report

# Get probability of attrition (Class 1)
y_pred_prob_gb = gb_model.predict_proba(X_test_scaled)[:, 1]

# Lower threshold from 0.5 to 0.35
threshold = 0.35
y_pred_gb_adjusted = (y_pred_prob_gb >= threshold).astype(int)

# Evaluate new predictions
print(f"\nðŸ”¹ Gradient Boosting with Lowered Threshold ({threshold})")
print(classification_report(y_test, y_pred_gb_adjusted))



import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve

# Define threshold range
thresholds = np.linspace(0.1, 0.9, 9)

precisions, recalls, f1_scores = [], [], []

for t in thresholds:
    y_pred_adj = (y_pred_prob_gb >= t).astype(int)
    report = classification_report(y_test, y_pred_adj, output_dict=True)
    precisions.append(report["1"]["precision"])
    recalls.append(report["1"]["recall"])
    f1_scores.append(report["1"]["f1-score"])

# Plot Precision-Recall tradeoff
plt.figure(figsize=(8, 5))
plt.plot(thresholds, precisions, marker="o", label="Precision")
plt.plot(thresholds, recalls, marker="o", label="Recall")
plt.plot(thresholds, f1_scores, marker="o", label="F1-Score")
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision, Recall, and F1-Score vs. Threshold")
plt.legend()
plt.show()




# Set the optimized threshold
optimal_threshold = 0.22  # Adjusted from 0.5 to 0.25

# Get predicted probabilities
y_pred_prob_gb = gb_model.predict_proba(X_test_scaled)[:, 1]

# Apply new threshold
y_pred_gb_final = (y_pred_prob_gb >= optimal_threshold).astype(int)

# Evaluate final Gradient Boosting performance
from sklearn.metrics import classification_report

print(f"\nðŸ”¹ Gradient Boosting with Optimized Threshold ({optimal_threshold})")
print(classification_report(y_test, y_pred_gb_final))

## XGB Model 



from xgboost import XGBClassifier

xgb_tuned = XGBClassifier(
    scale_pos_weight=6,  # Increased weight for attrition class
    max_depth=7, 
    learning_rate=0.03, 
    n_estimators=700,  
    colsample_bytree=0.8, 
    subsample=0.8,        
    random_state=42
)

xgb_tuned.fit(X_train_scaled, y_train_smote)
y_pred_xgb_tuned = xgb_tuned.predict(X_test_scaled)

print("\nðŸ”¹ Tuned XGBoost Performance")
print(classification_report(y_test, y_pred_xgb_tuned))



from lightgbm import LGBMClassifier

lgbm_model = LGBMClassifier(n_estimators=700, learning_rate=0.03, max_depth=7, class_weight="balanced", random_state=42)
lgbm_model.fit(X_train_scaled, y_train_smote)
y_pred_lgbm = lgbm_model.predict(X_test_scaled)

print("\nðŸ”¹ LightGBM Performance")
print(classification_report(y_test, y_pred_lgbm))




from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression

poly_log_reg = make_pipeline(
    PolynomialFeatures(degree=2),  # Add non-linear interactions
    LogisticRegression(class_weight="balanced", solver="liblinear")
)
poly_log_reg.fit(X_train_scaled, y_train_smote)
y_pred_poly_log = poly_log_reg.predict(X_test_scaled)

print("\nðŸ”¹ Polynomial Logistic Regression Performance")
print(classification_report(y_test, y_pred_poly_log))



from sklearn.neural_network import MLPClassifier

mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation="relu", solver="adam", max_iter=500, random_state=42)
mlp_model.fit(X_train_scaled, y_train_smote)
y_pred_mlp = mlp_model.predict(X_test_scaled)

print("\nðŸ”¹ Neural Network Performance")
print(classification_report(y_test, y_pred_mlp))



## Polinominial Model is looking the best 


from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

# Define pipeline
poly = PolynomialFeatures()  # Degree will be set in GridSearch
log_reg = LogisticRegression(solver="liblinear", max_iter=500)

pipeline = make_pipeline(poly, log_reg)

# Define hyperparameter grid
param_grid = {
    "polynomialfeatures__degree": [2, 3],  # Try quadratic and cubic terms
    "logisticregression__C": [0.1, 1, 10],  # Regularization strength
    "logisticregression__class_weight": [{0: 1, 1: 3}, {0: 1, 1: 4}, {0: 1, 1: 5}]  # Adjusting for class imbalance
}

# Grid search with cross-validation
grid_search = GridSearchCV(pipeline, param_grid, scoring="f1", cv=5, n_jobs=-1)
grid_search.fit(X_train_scaled, y_train_smote)

# Best model from tuning
best_poly_model = grid_search.best_estimator_

# Predict on test data
y_pred_poly_tuned = best_poly_model.predict(X_test_scaled)

# Evaluate the tuned model
from sklearn.metrics import classification_report

print("\nðŸ”¹ Best Polynomial Logistic Regression Model")
print("Best Parameters:", grid_search.best_params_)
print("\nClassification Report:")
print(classification_report(y_test, y_pred_poly_tuned))
